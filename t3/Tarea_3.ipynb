{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxrkZWMLZB8z"
      },
      "source": [
        "# Tarea 3: Sequence Labelling\n",
        "**Procesamiento de Lenguaje Natural (CC6205-1 - Oto√±o 2024)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b97b4IJjZGxM"
      },
      "source": [
        "## Tarjeta de identificaci√≥n\n",
        "\n",
        "**Nombres:**\n",
        "\n",
        "```- Ignacio Albornoz```\n",
        "\n",
        "```- Eduardo Silva```\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 06/06.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:** 4 horas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKcZMFlmZ3b9"
      },
      "source": [
        "## Instrucciones\n",
        "Bienvenid@s a la tercera tarea en el curso de Natural Language Processing (NLP). Esta tarea tiene como objetivo evaluar los contenidos te√≥ricos de las √∫ltimas semanas de clases posteriores a la tarea 2, enfocado principalmente en **Word Embeddings**, **Sequence Labeling - HMM**, **Convolutional Neural Networks** y **Recurrent Neural Networks**. Si a√∫n no has visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "La tarea consta de una una parte pr√°ctica con el f√≠n de introducirlos a la programaci√≥n en Python enfocada en NLP.\n",
        "\n",
        "* La tarea es en **grupo** (maximo hasta 3 personas).\n",
        "* La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "* El formato de entrega es este mismo Jupyter Notebook.\n",
        "* Al momento de la revisi√≥n su c√≥digo ser√° ejecutado. Por favor verifiquen que su entrega no tenga errores de compilaci√≥n.\n",
        "* Completar la tarjeta de identificaci√≥n. Sin ella no podr√° tener nota."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTrhOKraAw2"
      },
      "source": [
        "## Material de referencia\n",
        "\n",
        "Diapositivas del curso üìÑ\n",
        "    \n",
        "- [Word Embeddings](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-wordvectors.pdf)\n",
        "- [Sequence Labeling - HMM](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-HMM.pdf)\n",
        "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf)\n",
        "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf)\n",
        "\n",
        "Videos del curso üì∫\n",
        "\n",
        "- Word Embeddings: [Parte 1](https://www.youtube.com/watch?v=wtwUsJMC9CA), [Parte 2](https://www.youtube.com/watch?v=XDxzQ7JU95U), [Parte 3](https://www.youtube.com/watch?v=Ikyc3DRVodk)\n",
        "\n",
        "- Sequence Labeling - HMM: [Parte 1](https://www.youtube.com/watch?v=-ngfOZz8yK0), [Parte 2](https://www.youtube.com/watch?v=Tjgb-yQOg54), [Parte 3](https://www.youtube.com/watch?v=aaa5Qoi8Vco), [Parte 4](https://www.youtube.com/watch?v=4pKWIDkF_6Y)\n",
        "\n",
        "- [Convolutional Neural Networks](https://www.youtube.com/watch?v=lLZW5Fn40r8)\n",
        "\n",
        "- Recurrent Neural Networks: [Parte 1](https://www.youtube.com/watch?v=BmhjUkzz3nk), [Parte 2](https://www.youtube.com/watch?v=z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "025o3RYmm3oM"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "El objetivo de esta tarea es resolver una de las tareas m√°s importantes en el √°rea del procesamiento de lenguage natural, relacionada con la extracci√≥n de informaci√≥n: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf).\n",
        "\n",
        "En particular, deber√°n crear distintos modelos que apunten a resolver la tarea de NER en Espa√±ol. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el √°rea de NER en Espa√±ol y a√∫n m√°s en el contexto cl√≠nico, por ende puede ser considerado como una tarea bien desafiante y quiz√°s les interesa trabajar en el √°rea m√°s adelante en sus carreras.\n",
        "\n",
        "En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres m√©tricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n",
        "\n",
        "Como hemos estado viendo redes neuronales tanto en c√°tedras, tareas y auxiliares (o pr√≥ximamente lo har√°n), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla.\n",
        "\n",
        "Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (de todas maneras, como es un corpus nuevo, es dif√≠cil que haya alg√∫n modelo ya implementado con √©stas entidades)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBq7gcHnPTf"
      },
      "source": [
        "## Explicaci√≥n de NER\n",
        "\n",
        "En esta tarea van a resolver **NER**, com√∫nmente abordada como un problema de Sequence Labeling.\n",
        "\n",
        "**¬øQu√© es Sequence Labeling?**\n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (oraci√≥n) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qu√© se trata este problema.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n",
        "\n",
        "- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un n√∫mero o un s√≠mbolo.\n",
        "\n",
        "- *Entidad*: No es m√°s que un trozo de texto (uno o m√°s tokens) asociado a una categor√≠a predefinida. Originalmente se sol√≠an utilizar categor√≠as como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n",
        "\n",
        "- *L√≠mites de una entidad*: Son los √≠ndices de los tokens de inicio y f√≠n dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: Es la categor√≠a predefinida asociada a la entidad.\n",
        "\n",
        "Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, $e son los l√≠mites de la entidad (√≠ndices de los tokens de inicio y fin, respectivamente) y $t corresponde al tipo de entidad o categor√≠a. Ya veremos m√°s ejemplos luego de describir el Dataset.\n",
        "\n",
        "**Corpus de la Lista de espera**\n",
        "\n",
        "Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber m√°s sobre c√≥mo fueron generados los datos pueden revisar el paper publicado hace unos meses atr√°s en el workshop de EMNLP, una de las conferencias m√°s importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n",
        "\n",
        "Este corpus Chileno est√° constituido originalmente por 7 tipos de entidades pero por simplicidad en esta tarea trabajar√°n con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication**\n",
        "- **Procedures**\n",
        "- **Family_Member**\n",
        "\n",
        "Si quieren obtener m√°s informaci√≥n sobre estas entidades pueden consultar la [gu√≠a de anotaci√≥n](https://plncmm.github.io/annodoc/). Adem√°s, mencionar que este corpus est√° restringido bajo una licencia que permite solamente su uso acad√©mico, as√≠ que no puede ser compartido m√°s all√° de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este √∫ltimo es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los t√©rminos y condiciones de la tarea est√°n de acuerdo con los puntos descritos anteriormente.\n",
        "\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos que ser√°n entregados a ustedes vienen en un formato est√°ndar utilizado en NER, llamado ConLL. No es m√°s que un archivo de texto, que cumple las siguientes propiedades.\n",
        "\n",
        "- Un salto de linea corresponde a la separaci√≥n entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, m√°s conocidos como batches.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partici√≥n.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato cl√°sico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token est√° asociado a la categor√≠a O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Aqu√≠ va un ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Seg√∫n nuestra definici√≥n tenemos las siguientes tres entidades (enumerando desde 0):\n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "Repasen un par de veces todos estos conceptos antes de pasar a la siguiente secci√≥n del notebook.\n",
        "Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **m√©trica estricta**. Esta m√©trica se llama as√≠ ya que considera correcta una predicci√≥n de su modelo, s√≥lo si al compararlo con las entidades reales **coinciden tanto los l√≠mites de la entidad como el tipo.**\n",
        "\n",
        "Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una m√©trica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la tarea:\n",
        "\n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf): [Parte 1](https://youtu.be/BmhjUkzz3nk), [Parte 2](https://youtu.be/z43YFR1iIvk), [Parte 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgaywGdo0bh"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook est√° programado en la librer√≠a [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si as√≠ lo desean. El c√≥digo contiene lo siguiente:\n",
        "\n",
        "- La carga de los datasets, creaci√≥n de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs).\n",
        "\n",
        "- La implementaci√≥n b√°sica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad.\n",
        "\n",
        "- La construcci√≥n del formato del output requerido para que lo puedan probar en la tarea en codalab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfMlGTCgIq8o"
      },
      "source": [
        "### **Sugerencias**\n",
        "Se espera que ustedes puedan experimentar con el baseline utilizando (pero no limit√°ndose) estas sugerencias:\n",
        "\n",
        "*   Probar la t√©cnica de early stopping.\n",
        "*   Variar la cantidad de par√°metros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de par√°metros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en espa√±ol aqu√≠](https://github.com/dccuchile/spanish-word-embeddings). Tambi√©n aqu√≠ pueden encontrar unos embeddings cl√≠nicos en Espa√±ol: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n",
        "*   Variar la cantidad de √©pocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, etc.\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU.\n",
        "*   Probar usando capas de atenci√≥n.\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en espa√±ol usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWEFCu4_sWwm"
      },
      "source": [
        "## Experimento base\n",
        "\n",
        "El c√≥digo que les entregaremos servir√° de baseline para luego implementar mejores modelos.\n",
        "En general, el c√≥digo asociado a la carga de los datos, las funciones de entrenamiento, de evaluaci√≥n y la predicci√≥n de los datos de la tarea no deber√≠an cambiar.\n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperpar√°metros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v9-YCA9sg-A"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librer√≠a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librer√≠a tuvo cambios radicales, quedando las funcionalidades pasadas depreciadas de la librer√≠a ```legacy```. Esto ya que si quieren usar m√°s funciones de la librer√≠a entonces vean los cambios en la documentaci√≥n debe usar la versi√≥n antigua con python 3.8\n",
        "\n",
        "El proceso ser√° el siguiente:\n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Cargar los datasets con la clase ```TaggingDataset``` de m√°s abajo.\n",
        "3. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLZkrIVQsqBB",
        "outputId": "d7e07262-747a-488a-d2aa-34e5d1325a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4DCjj5bsxNJ",
        "outputId": "38f23831-eaa0-4b91-b990-d44a4504bba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDCqI-1is2j_"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OIMlUsp3s7J-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn_YMpyetNhn",
        "outputId": "ee7979ff-ea47-42e1-a05e-6749086bc69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "# NUEVO DATALOADER Y OTRAS COSAS NECESARIAS\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "class TaggingDataset(Dataset):\n",
        "    def __init__(self, paths, lower=False, separator=\" \", encoding=\"utf-8\"):\n",
        "\n",
        "        data = []\n",
        "        for path in paths:\n",
        "          with open(path, 'r', encoding=encoding) as file:\n",
        "            text, tag = [], []\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    data.append(dict({'text':text, 'nertags':tag}))\n",
        "                    text, tag = [], []\n",
        "                else:\n",
        "                    line_content = line.split(separator) # .rstrip('\\n')\n",
        "                    if lower:\n",
        "                      text.append(line_content[0].lower())\n",
        "                    else:\n",
        "                      text.append(line_content[0])\n",
        "                    tag.append(line_content[1])\n",
        "          data.append(dict({'text':text, 'nertags':tag}))\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        text = item[\"text\"]\n",
        "        nertags = item[\"nertags\"]\n",
        "        return nertags, text\n",
        "\n",
        "def fit_vocab(data_iter):\n",
        "\n",
        "  def update_counter(counter_obj):\n",
        "    sorted_by_freq_tuples = sorted(counter_obj.items(),\n",
        "                                  key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "    return ordered_dict\n",
        "\n",
        "  counter_1 = Counter()\n",
        "  counter_2 = Counter()\n",
        "  for _nertags, _text in data_iter:\n",
        "    counter_1.update(_text)\n",
        "    counter_2.update(_nertags)\n",
        "\n",
        "  od1 = update_counter(counter_1)\n",
        "  od2 = update_counter(counter_2)\n",
        "\n",
        "  v1 = vocab(od1, specials=['<PAD>', '<unk>'])\n",
        "  v1.set_default_index(v1[\"<unk>\"])\n",
        "  v2 = vocab(od2, specials=['<PAD>'])\n",
        "\n",
        "  text_pipeline = lambda x: v1(x)\n",
        "  nertags_pipeline = lambda x: v2(x)\n",
        "\n",
        "  return text_pipeline, nertags_pipeline, v1, v2\n",
        "\n",
        "def collate_batch(batch, nertags_pipeline, text_pipeline, device):\n",
        "  nertags_list, text_list = [], []\n",
        "  for _nertags, _text in batch:\n",
        "    processed_nertags = torch.tensor(nertags_pipeline(_nertags),\n",
        "                                     dtype=torch.int64)\n",
        "    nertags_list.append(processed_nertags)\n",
        "    processed_text = torch.tensor(text_pipeline(_text),\n",
        "                                  dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "  nertags_list = pad_sequence(nertags_list, batch_first=True).T\n",
        "  text_list = pad_sequence(text_list, batch_first=True).T\n",
        "  return nertags_list.to(device), text_list.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ctd5gNmzxnnE"
      },
      "outputs": [],
      "source": [
        "data_iter = TaggingDataset([\"train.txt\", \"dev.txt\"])\n",
        "\n",
        "data_length = len(data_iter)\n",
        "\n",
        "train_length = int(data_length * 0.8)\n",
        "dev_length = int(data_length * 0.1)\n",
        "test_length = data_length - train_length - dev_length\n",
        "\n",
        "train_iter, dev_iter, test_iter = torch.utils.data.random_split(\n",
        "    data_iter,\n",
        "     (train_length, dev_length, test_length),\n",
        "    torch.Generator().manual_seed(42))\n",
        "\n",
        "text_pipeline, nertags_pipeline, TEXT, NER_TAGS = fit_vocab(train_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-MPvr5LDkq5",
        "outputId": "9a4c6059-c09c-4485-cc21-670969e608d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7132, 891, 893)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(train_iter), len(dev_iter), len(test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h-BjEO1exsuq"
      },
      "outputs": [],
      "source": [
        "# seteamos algunos valores de interes\n",
        "UNK_IDX = TEXT.vocab.get_stoi()['<unk>']\n",
        "PAD_IDX = TEXT.vocab.get_stoi()['<PAD>']\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.get_stoi()['<PAD>']\n",
        "O_TAG_IDX = NER_TAGS.vocab.get_stoi()['O']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD3AmVkOxsE8",
        "outputId": "fd5b0511-fa2c-4e67-eed4-38ffc42b41c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 22\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "fix_collocate_batch = lambda x: collate_batch(x, nertags_pipeline, text_pipeline, device)\n",
        "\n",
        "dataloader_train = DataLoader(\n",
        "    train_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",
        ")\n",
        "dataloader_dev = DataLoader(\n",
        "    dev_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",
        ")\n",
        "dataloader_test = DataLoader(\n",
        "    test_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fix_collocate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3XrV_YyyBWB",
        "outputId": "c08ad5a6-37b2-41b0-9414-99ab83afba63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-Disease',\n",
              "  'I-Disease',\n",
              "  'I-Disease',\n",
              "  'I-Disease',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>'],\n",
              " ['SOLICITO',\n",
              "  'EVALUACION',\n",
              "  'Y',\n",
              "  'TTO',\n",
              "  'SUBLUXACION',\n",
              "  'ATM',\n",
              "  'IZQ',\n",
              "  '.',\n",
              "  ',',\n",
              "  'DOLOR',\n",
              "  'APERTURA',\n",
              "  '.',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>',\n",
              "  '<PAD>'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "example = next(iter(dataloader_train))\n",
        "ner_tags_example = example[0]\n",
        "text_example = example[1]\n",
        "\n",
        "# revisamos el primer ejemplo\n",
        "[NER_TAGS.vocab.get_itos()[j] for j in ner_tags_example[:, 1]], [TEXT.vocab.get_itos()[j] for j in text_example[:, 1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D81QD5Ky2_4",
        "outputId": "3dc007a9-1e3e-4fc2-a51c-1c39c6a64f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m494.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=6f6efe9ffdd197408e5c9b9f0b540680e91b54657907a80f635185a226891c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YdWPWTMTy8Cm"
      },
      "outputs": [],
      "source": [
        "# Definimos las m√©tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.get_itos()[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.get_itos()[v] for v in y_true]]\n",
        "\n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN984Z0XzHkP"
      },
      "source": [
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr√° una capa de embedding, unas cuantas LSTM y una capa de salida y usar√° dropout en el entrenamiento.\n",
        "\n",
        "Este constar√° de los siguientes pasos:\n",
        "\n",
        "1. Definir la clase que contendr√° la red.\n",
        "2. Definir los hiperpar√°metros e inicializar la red.\n",
        "3. Definir el n√∫mero de √©pocas de entrenamiento\n",
        "4. Definir la funci√≥n de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G3vNCpAyy7_L"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        # embedded = self.dropout(self.embedding(text))\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        #predictions = self.fc(self.dropout(outputs))\n",
        "        predictions = self.fc(outputs)\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ofso-aozeSL"
      },
      "source": [
        "Hiperpar√°metros de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9ryN9aKzy773"
      },
      "outputs": [],
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.6\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado...\n",
        "\n",
        "baseline_n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGxRLBwzhsg"
      },
      "source": [
        "Definimos la funci√≥n de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LtfG9RUQy7vx"
      },
      "outputs": [],
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2Lm8Yszu11"
      },
      "source": [
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el n√∫mero de √©pocas de entrenamiento, la loss y el optimizador que usar√°n para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1-TnXy2LzvXO"
      },
      "outputs": [],
      "source": [
        "model = baseline_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wQq18i0Ehc"
      },
      "source": [
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribuci√≥n normal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kgtnu2L0E-B",
        "outputId": "15096509-aff1-4f84-9cc0-f815427f1336"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_RNN(\n",
              "  (embedding): Embedding(16496, 300, padding_idx=0)\n",
              "  (lstm): LSTM(300, 256, num_layers=3, dropout=0.6)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.6, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2TB3-J80aBB",
        "outputId": "a3ada64a-d2b9-425a-9b91-51808ef3d8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 6,575,948 par√°metros entrenables.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnuk_Hs10djW"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qJhs1OX0gls"
      },
      "source": [
        "Definimos el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xBAYockO0mE7"
      },
      "outputs": [],
      "source": [
        "# Optimizador\n",
        "\n",
        "optimizer = optim.Adam(model.parameters()) # SGD, AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGj6tfdd0qJb"
      },
      "source": [
        "Enviamos el modelo a cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_68iNmcN0qkm"
      },
      "outputs": [],
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuKvsbrw0wHw"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos:\n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracci√≥n de la √©poca. Se utilizan para entrenar mas r√°pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci√≥n del backpropagation)\n",
        "\n",
        "Esta funci√≥n est√° encargada de entrenar la red en una √©poca. Para esto, por cada batch de la √©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observaci√≥n: En algunos comentarios aparecer√° el tama√±o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ShIJbx2v0wlf"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la √©poca:\n",
        "    for tags, text in iterator:\n",
        "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text.to(device))\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        #ipdb.set_trace()\n",
        "        tags = torch.reshape(tags, (-1,)).to(device)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "\n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los par√°metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las m√©tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return ( epoch_loss / len(iterator), epoch_precision / len(iterator),\n",
        "              epoch_recall / len(iterator), epoch_f1 / len(iterator) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWSosAit0xCJ"
      },
      "source": [
        "#### **Definimos la funci√≥n de evaluaci√≥n**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaci√≥n.\n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las m√©tricas asociadas al conjunto de validaci√≥n.\n",
        "Ya que las m√©tricas son calculadas por cada batch, estas son retornadas promediadas por el n√∫mero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z3fRycaX0xbA"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for tags, text in iterator:\n",
        "            # Predecimos\n",
        "            predictions = model(text.to(device))\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = torch.reshape(tags, (-1,)).to(device)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las m√©tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las m√©tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tA8nad2A1HUm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2twwfpmz1E7K"
      },
      "source": [
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de c√≥digo ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el n√∫mero de √©pocas y luego por cada √©poca, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez.\n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funci√≥n `init_weights`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Py5sCE1FTe",
        "outputId": "9aae85ec-348a-4266-eaa3-935c82981db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <PAD> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.836 | Train f1: 0.36 | Train precision: 0.54 | Train recall: 0.29\n",
            "\t Val. Loss: 0.496 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.61\n",
            "Epoch: 02 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.397 | Train f1: 0.71 | Train precision: 0.76 | Train recall: 0.68\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.71 |  Val. precision: 0.72 | Val. recall: 0.71\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.258 | Train f1: 0.81 | Train precision: 0.82 | Train recall: 0.80\n",
            "\t Val. Loss: 0.404 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.71\n",
            "Epoch: 04 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.191 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.435 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.71\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.145 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.500 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.70\n",
            "Epoch: 06 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.117 | Train f1: 0.92 | Train precision: 0.91 | Train recall: 0.92\n",
            "\t Val. Loss: 0.523 |  Val. f1: 0.71 |  Val. precision: 0.73 | Val. recall: 0.70\n",
            "Epoch: 07 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.101 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.540 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.71\n",
            "Epoch: 08 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.087 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.578 |  Val. f1: 0.71 |  Val. precision: 0.71 | Val. recall: 0.71\n",
            "Epoch: 09 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.614 |  Val. f1: 0.70 |  Val. precision: 0.69 | Val. recall: 0.72\n",
            "Epoch: 10 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.650 |  Val. f1: 0.70 |  Val. precision: 0.67 | Val. recall: 0.73\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: dataloader_train y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, dataloader_train, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci√≥n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, dataloader_dev, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMU8i9eV-xYq"
      },
      "source": [
        "**Importante**: Recuerden que el √∫ltimo modelo entrenado no es el mejor (probablemente est√© *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaci√≥n. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqJU5AEY-bwK",
        "outputId": "b4b5213c-14ad-49bb-c1ff-96766912ea94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JX84ETMg-1Qv"
      },
      "outputs": [],
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d9Yfz95_GIz"
      },
      "source": [
        "#### **Evaluamos el set de validaci√≥n con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaci√≥n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6mMnQIC-2G2",
        "outputId": "cc208646-c49a-4cb6-bab6-67e21f26d66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.404 |  Val. f1: 0.72 | Val. precision: 0.74 | Val. recall: 0.71\n"
          ]
        }
      ],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7FzQVxK-2kc"
      },
      "source": [
        "#### **Evaluamos el set de prueba con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de prueba con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meu3CsgT-3PE",
        "outputId": "7a328c21-d8de-410f-c7f0-6820371c3b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test. Loss: 0.435 |  Test. f1: 0.71 | Test. precision: 0.73 | Test. recall: 0.69\n"
          ]
        }
      ],
      "source": [
        "prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n",
        "    model, dataloader_test, criterion)\n",
        "\n",
        "print(\n",
        "    f'Test. Loss: {prueba_loss:.3f} |  Test. f1: {prueba_f1:.2f} | Test. precision: {prueba_precision:.2f} | Test. recall: {prueba_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2bRYNRnKRzK"
      },
      "source": [
        "## Experimentos\n",
        "\n",
        "En esta secci√≥n deberan explicar, implementar y evaluar **TRES EXPERIMENTO DIFERENTES**. Puede revisar la [lista de sugerencias](#scrollTo=TfMlGTCgIq8o) al inicio del enunciado para sacar ideas de experimentos. Se espera que los experimentos sean relevantes y desafiantes.\n",
        "\n",
        "Cada experimento debe contener las siguientes cuatro subsecciones:\n",
        "\n",
        "- *Explicaci√≥n del experimento*: debe motivar y explicar en que consiste su experimento. La explicaci√≥n debe fundamentar porque es un experimento relevante y desafiantes, ademas de aclarar que estudiaran.\n",
        "\n",
        "- *Implementacion del experimento*: debe implementar su experimento con codigo debidamente comentado, con tal de que sea legible.\n",
        "\n",
        "- *Evaluar el experimento*: debe evaluar con las metricas propuestas para generar los resultados obtenidos con sus experimentos sobre el conjunto de prueba.\n",
        "\n",
        "- *Analizar el experimento*: debe hacer un breve analisis de los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwdOcPfbL3pH"
      },
      "source": [
        "### **Experimento 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkn38sTsdpzR"
      },
      "source": [
        "#### Explicaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Cr3jl1d0F3"
      },
      "source": [
        "En este experimento se a√±adiran 2 nuevas capas al baseline para mejorar la tarea de reconocimiento de Entidades Nombradas en el baseline. Estas son:\n",
        "\n",
        "- Capa de atenci√≥n: Esta capa calcula pesos de atenci√≥n, indicando que partes del input son m√°s relevantes para la tarea actual.\n",
        "\n",
        "- Capa de normalizaci√≥n: Una capa de normalizaci√≥n se utiliza en redes neuronales para normalizar las activaciones entre capas. Esto ayuda a mitigar los problemas de inestabilidad durante el entrenamiento y acelera la convergencia.\n",
        "\n",
        "Adem√°s se agrego un drop out de 0.3.\n",
        "\n",
        "En particular, nos interesa observar el efecto de la capa de atenci√≥n en la tarea a resolver.\n",
        "\n",
        "Utilizar capas de atenci√≥n en modelos para resolver problemas NER es altamente beneficioso debido a su capacidad para capturar dependencias a largo plazo entre palabras dentro de una oraci√≥n, facilitando as√≠ la identificaci√≥n precisa de entidades nombradas. Estas capas permiten al modelo enfocarse selectivamente en partes espec√≠ficas de la secuencia de entrada, lo que es crucial para gestionar contextos complejos y resolver ambig√ºedades sint√°cticas y sem√°nticas. Adem√°s, en el panorama actual del aprendizaje profundo, las capas de atenci√≥n han ganado prominencia significativa, especialmente con el desarrollo de modelos como los Transformers, que dependen en gran medida de mecanismos de atenci√≥n para procesar eficientemente grandes cantidades de datos secuenciales y capturar relaciones complejas. Esta tecnolog√≠a no solo mejora el rendimiento del modelo en t√©rminos de precisi√≥n, sino que tambi√©n facilita la interpretaci√≥n y la explicabilidad, permitiendo visualizar qu√© partes de la entrada son m√°s relevantes para las decisiones del modelo, aspecto fundamental en aplicaciones sensibles como el an√°lisis de sentimientos, diagn√≥stico m√©dico y NER.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5dlEgEqdtvp"
      },
      "source": [
        "#### Implementaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # hidden = [batch size, hid dim]\n",
        "        # encoder_outputs = [src len, batch size, hid dim]\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # repeat hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        # hidden = [batch size, src len, hid dim]\n",
        "        # encoder_outputs = [batch size, src len, hid dim]\n",
        "\n",
        "        energy = torch.tanh(self.attn(encoder_outputs))\n",
        "\n",
        "        # energy = [batch size, src len, hid dim]\n",
        "\n",
        "        energy = energy.permute(0, 2, 1)\n",
        "\n",
        "        # energy = [batch size, hid dim, src len]\n",
        "\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "\n",
        "        # v = [batch size, 1, hid dim]\n",
        "\n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "\n",
        "        # attention = [batch size, src len]\n",
        "\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        return torch.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "j1hUtPuPa-b0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_RNN_Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            dropout=dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.attention = Attention(hidden_dim * 2 if bidirectional else hidden_dim)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2 if bidirectional else hidden_dim)\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        # Mover text al mismo dispositivo que el embedding\n",
        "        text = text.to(self.embedding.weight.device)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        outputs = self.layer_norm(outputs)\n",
        "\n",
        "        # aplicar mecanismo de atenci√≥n\n",
        "        attention_weights = self.attention(hidden[-1], outputs)\n",
        "        attention_weights = attention_weights.unsqueeze(2)\n",
        "\n",
        "        # aplicar pesos de atenci√≥n\n",
        "        attended_outputs = outputs.permute(1, 0, 2) * attention_weights\n",
        "        attended_outputs = attended_outputs.permute(1, 0, 2)\n",
        "\n",
        "        emissions = self.fc(attended_outputs)\n",
        "\n",
        "        return emissions\n"
      ],
      "metadata": {
        "id": "H-sBZ-aUbCXa"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dhuuMKVAdvqt"
      },
      "outputs": [],
      "source": [
        "### Escribir implenetaci√≥n desde aqu√≠\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "\n",
        "\n",
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.3\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "exp_1 = NER_RNN_Attention(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "baseline_model_name = 'experimento_1'  # nombre que tendr√° el modelo guardado...\n",
        "\n",
        "baseline_n_epochs = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_experimento_1 = exp_1\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ],
      "metadata": {
        "id": "Z-OQJOsObYZG"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_experimento_1.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4MeREpcbZ_8",
        "outputId": "c92ecffc-f113-4c1f-f2e1-3dfe8fdf6a70"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_RNN_Attention(\n",
              "  (embedding): Embedding(16496, 300, padding_idx=0)\n",
              "  (lstm): LSTM(300, 256, num_layers=3, dropout=0.3)\n",
              "  (attention): Attention(\n",
              "    (attn): Linear(in_features=256, out_features=256, bias=True)\n",
              "  )\n",
              "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model_experimento_1.parameters()) # SGD, AdamW"
      ],
      "metadata": {
        "id": "mDQwLPUSbb09"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n",
        "model_experimento_1 = model_experimento_1.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "G7OUIUJTbccE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmOWuw0owymT",
        "outputId": "168063b4-59a8-4d28-c038-e322e73e93d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 1.636 | Train f1: 0.01 | Train precision: 0.09 | Train recall: 0.01\n",
            "\t Val. Loss: 1.052 |  Val. f1: 0.04 |  Val. precision: 0.30 | Val. recall: 0.02\n",
            "Epoch: 02 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.874 | Train f1: 0.30 | Train precision: 0.55 | Train recall: 0.22\n",
            "\t Val. Loss: 0.751 |  Val. f1: 0.44 |  Val. precision: 0.57 | Val. recall: 0.37\n",
            "Epoch: 03 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.558 | Train f1: 0.60 | Train precision: 0.68 | Train recall: 0.53\n",
            "\t Val. Loss: 0.563 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.60\n",
            "Epoch: 04 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.376 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.71\n",
            "\t Val. Loss: 0.516 |  Val. f1: 0.68 |  Val. precision: 0.70 | Val. recall: 0.67\n",
            "Epoch: 05 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.280 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.80\n",
            "\t Val. Loss: 0.502 |  Val. f1: 0.71 |  Val. precision: 0.74 | Val. recall: 0.68\n",
            "Epoch: 06 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.217 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.85\n",
            "\t Val. Loss: 0.507 |  Val. f1: 0.71 |  Val. precision: 0.72 | Val. recall: 0.70\n",
            "Epoch: 07 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.173 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.88\n",
            "\t Val. Loss: 0.529 |  Val. f1: 0.70 |  Val. precision: 0.71 | Val. recall: 0.70\n",
            "Epoch: 08 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.145 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
            "\t Val. Loss: 0.545 |  Val. f1: 0.70 |  Val. precision: 0.69 | Val. recall: 0.72\n",
            "Epoch: 09 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.123 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.600 |  Val. f1: 0.70 |  Val. precision: 0.69 | Val. recall: 0.72\n",
            "Epoch: 10 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.105 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.671 |  Val. f1: 0.72 |  Val. precision: 0.71 | Val. recall: 0.72\n",
            "Epoch: 11 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.090 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.633 |  Val. f1: 0.71 |  Val. precision: 0.70 | Val. recall: 0.72\n",
            "Epoch: 12 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.081 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.706 |  Val. f1: 0.72 |  Val. precision: 0.73 | Val. recall: 0.72\n",
            "Epoch: 13 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.073 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.714 |  Val. f1: 0.71 |  Val. precision: 0.71 | Val. recall: 0.72\n",
            "Epoch: 14 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.067 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.670 |  Val. f1: 0.72 |  Val. precision: 0.71 | Val. recall: 0.73\n",
            "Epoch: 15 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.060 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.709 |  Val. f1: 0.72 |  Val. precision: 0.72 | Val. recall: 0.73\n",
            "Epoch: 16 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.053 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.824 |  Val. f1: 0.71 |  Val. precision: 0.71 | Val. recall: 0.72\n",
            "Epoch: 17 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.052 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.808 |  Val. f1: 0.71 |  Val. precision: 0.71 | Val. recall: 0.72\n",
            "Epoch: 18 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.865 |  Val. f1: 0.72 |  Val. precision: 0.73 | Val. recall: 0.71\n",
            "Epoch: 19 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.045 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.698 |  Val. f1: 0.71 |  Val. precision: 0.71 | Val. recall: 0.71\n",
            "Epoch: 20 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.043 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.830 |  Val. f1: 0.72 |  Val. precision: 0.72 | Val. recall: 0.72\n"
          ]
        }
      ],
      "source": [
        "#Entrenamiento\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: dataloader_train y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model_experimento_1, dataloader_train, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci√≥n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model_experimento_1, dataloader_dev, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model_experimento_1.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model_experimento_1.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5KvQqEZbi2E",
        "outputId": "ca0251ec-e6ca-4479-d5b8-20abb9516487"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wMbKS1ELbkXd"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37IjYq-_d5Pv"
      },
      "source": [
        "#### Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "kGz_8OB7eItO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bce335-5166-47f6-a451-11e69da4985d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.502 |  Val. f1: 0.70 | Val. precision: 0.74 | Val. recall: 0.67\n"
          ]
        }
      ],
      "source": [
        "prueba_loss, prueba_precision, prueba_recall, prueba_f1 = evaluate(\n",
        "    model_experimento_1, dataloader_test, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {prueba_loss:.3f} |  Val. f1: {prueba_f1:.2f} | Val. precision: {prueba_precision:.2f} | Val. recall: {prueba_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pURRQLkReNMY"
      },
      "source": [
        "#### An√°lisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfTrOktPeSB7"
      },
      "source": [
        "Observando los resultados se puede comprobar que el modelo del experimento 1 solo supera al baseline en la precisi√≥n.\n",
        "\n",
        "Esto pudo ocurrir dado que las relaciones entre entidades nombradas y sus contextos no requieren de una atenci√≥n detallada o si la estructura del problema es m√°s dependiente de caracter√≠sticas locales o gramaticales que de relaciones globales dentro de las secuencias. Adem√°s, la implementaci√≥n incorrecta de las capas de atenci√≥n, como la elecci√≥n inapropiada de hiperpar√°metros o la falta de suficiente cantidad de datos de entrenamiento, pudo llevar a un rendimiento inferior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFrDPaGej4y"
      },
      "source": [
        "### **Experimento 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-dsNSEdej4z"
      },
      "source": [
        "#### Explicaci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JD-PzGej4z"
      },
      "source": [
        "En este experimento, se profundiza en la optimizaci√≥n de hiperpar√°metros para un modelo de Reconocimiento de Entidades Nombradas (NER) empleando una t√©cnica de b√∫squeda en grilla. Exploraremos sistem√°ticamente la influencia de variar la dimensi√≥n de los embeddings, la dimensi√≥n de las capas ocultas LSTM, el n√∫mero de capas LSTMy la configuraci√≥n de bidireccionalidad en el rendimiento del modelo. Cada uno de estos par√°metros tiene un impacto significativo en c√≥mo el modelo aprende de los datos:\n",
        "\n",
        "- Dimensi√≥n de Embedding (EMBEDDING_DIM): Afecta directamente la capacidad del modelo para representar y diferenciar caracter√≠sticas de los tokens en un espacio dimensional. Un espacio m√°s amplio puede capturar m√°s detalles, pero tambi√©n puede provocar overfitting si es demasiado grande.\n",
        "\n",
        "- Dimensi√≥n de las Capas LSTM (HIDDEN_DIM): Determina el tama√±o de las unidades ocultas dentro de las capas LSTM. Unidades m√°s grandes pueden modelar mejor la informaci√≥n y las dependencias temporales en los datos, pero incrementan el riesgo de overfitting y el costo computacional.\n",
        "\n",
        "- N√∫mero de Capas LSTM (N_LAYERS): Incrementar el n√∫mero de capas puede ayudar a aprender representaciones m√°s complejas y abstractas de los datos. Sin embargo, tambi√©n hace que el modelo sea m√°s propenso al overfitting y m√°s dif√≠cil de entrenar.\n",
        "\n",
        "- Bidireccionalidad (BIDIRECTIONAL): Permite que las capas LSTM procesen la informaci√≥n en ambas direcciones (pasado y futuro) y es particularmente √∫til para comprender el contexto en problemas de secuencia como NER. Activar esta caracter√≠stica puede mejorar significativamente la precisi√≥n del modelo a costa de duplicar los recursos computacionales necesarios.\n",
        "\n",
        "El objetivo del experimento es identificar la combinaci√≥n √≥ptima de estos hiperpar√°metros que maximice la eficacia del modelo, medido a trav√©s de m√©tricas como el F1-score, precisi√≥n y recall. Esta b√∫squeda ayudar√° a mejorar la capacidad del modelo para procesar y etiquetar correctamente entidades en textos cl√≠nicos, proporcionando una herramienta m√°s robusta y precisa para la extracci√≥n de informaci√≥n.\n",
        "\n",
        "Notar que se decidi√≥ experimentar en una primera instancia, no usar dropout y adelantando los resultados se dej√≥ as√≠ ya que no fue necesario porque se variando el resto de hiperpar√°metros se logr√≥ mejorar las 3 m√©tricas objetivo F1-score, Precision y Recall. Adem√°s por una cuesti√≥n de tiempo solo se experiment√≥ con la birideccionalidad activada ya que la hip√≥tesis te√≥rica dicta que deb√≠a mejorar las m√©tricas, y commo ya dijimos se encontr√≥ una combinaci√≥n de hiperpar√°metros con la birideccionalidad activada que mejoraba las 3 m√©tricas objetivas no fue necesario probar con la bidireccionalidad desactivada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKZIGOUpej4z"
      },
      "source": [
        "#### Implementaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yBNY2J6aKT0",
        "outputId": "a74e3376-1361-418c-f205-f30b59a6ccd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probando configuraci√≥n: {'bidirectional': True, 'dropout': 0, 'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2}\n",
            "Val. Loss: 0.397 | Val. F1: 0.72\n",
            "Val. Loss: 0.349 | Val. F1: 0.75\n",
            "Val. Loss: 0.370 | Val. F1: 0.76\n",
            "Val. Loss: 0.480 | Val. F1: 0.75\n",
            "Val. Loss: 0.481 | Val. F1: 0.77\n",
            "Val. Loss: 0.523 | Val. F1: 0.76\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 42\u001b[0m     train_loss, train_precision, train_recall, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     valid_loss, valid_precision, valid_recall, valid_f1 \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     45\u001b[0m         model, dataloader_dev, criterion)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal. Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val. F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Por cada batch del iterador de la √©poca:\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tags, text \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Reiniciamos los gradientes calculados en la iteraci√≥n anterior\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#text = [sent len, batch size]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Predecimos los tags del texto del batch.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_t3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_t3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_t3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[0;32m----> 6\u001b[0m fix_collocate_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcollate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnertags_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m dataloader_train \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      9\u001b[0m     train_iter, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mfix_collocate_batch\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m dataloader_dev \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     12\u001b[0m     dev_iter, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mfix_collocate_batch\n\u001b[1;32m     13\u001b[0m )\n",
            "Cell \u001b[0;32mIn[4], line 80\u001b[0m, in \u001b[0;36mcollate_batch\u001b[0;34m(batch, nertags_pipeline, text_pipeline, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m nertags_list \u001b[38;5;241m=\u001b[39m pad_sequence(nertags_list, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     79\u001b[0m text_list \u001b[38;5;241m=\u001b[39m pad_sequence(text_list, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnertags_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, text_list\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import copy\n",
        "'''\n",
        "EMBEDDING_DIM_EXP = 400  # Nueva dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM_EXP = 512     # Nueva dimensi√≥n de las capas LSTM.\n",
        "N_LAYERS_EXP = 4         # Nuevo n√∫mero de capas LSTM.\n",
        "BIDIRECTIONAL_EXP = True # Habilitar la bidireccionalidad.\n",
        "'''\n",
        "# Definimos un diccionario con los rangos de los hiperpar√°metros que queremos probar\n",
        "param_grid = {\n",
        "    'embedding_dim': [200, 300, 400],\n",
        "    'hidden_dim': [256, 512],\n",
        "    'n_layers': [2, 3, 4],\n",
        "    'dropout': [0],\n",
        "    'bidirectional': [True]\n",
        "}\n",
        "\n",
        "# Creamos la cuadr√≠cula de par√°metros\n",
        "grid = ParameterGrid(param_grid)\n",
        "\n",
        "# Mejor modelo inicial\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "best_params = {}\n",
        "\n",
        "# Evaluamos cada combinaci√≥n de hiperpar√°metros\n",
        "for params in grid:\n",
        "    print(f\"Probando configuraci√≥n: {params}\")\n",
        "\n",
        "    # Establece la dimensi√≥n global de embedding para esta configuraci√≥n\n",
        "    global EMBEDDING_DIM\n",
        "    EMBEDDING_DIM = params['embedding_dim']\n",
        "\n",
        "    model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM,\n",
        "                    params['n_layers'], params['bidirectional'], params['dropout'], PAD_IDX)\n",
        "\n",
        "    model.apply(init_weights)\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_precision, train_recall, train_f1 = train(\n",
        "            model, dataloader_train, optimizer, criterion)\n",
        "        valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "            model, dataloader_dev, criterion)\n",
        "\n",
        "        print(f'Val. Loss: {valid_loss:.3f} | Val. F1: {valid_f1:.2f}')\n",
        "\n",
        "    if valid_f1 > best_f1:\n",
        "        best_f1 = valid_f1\n",
        "        best_model = copy.deepcopy(model)\n",
        "        best_params = params\n",
        "        torch.save(best_model.state_dict(), 'best_model_exp2.pt')\n",
        "\n",
        "print(f\"Mejor F1: {best_f1:.2f} con configuraci√≥n: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0UL1pPJaKT0",
        "outputId": "b82528f1-778a-42b4-e1ec-fb58134884da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los mejores par√°metros han sido guardados a 'best_params.json'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# Convertir el diccionario de los mejores par√°metros a un string JSON\n",
        "params_json = json.dumps(best_params, indent=4)\n",
        "\n",
        "# Guardar el string JSON en un archivo\n",
        "with open('best_params_exp2.json', 'w') as json_file:\n",
        "    json_file.write(params_json)\n",
        "\n",
        "print(\"Los mejores par√°metros han sido guardados a 'best_params_exp2.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFWJNRGjaKT0"
      },
      "outputs": [],
      "source": [
        "print(f'Hiperpar√°metros del mejor modelo encontrado: {best_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epaoMVyMej40"
      },
      "source": [
        "#### Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF8M8iLGej40",
        "outputId": "06fb82a0-0433-461e-f406-5f4782f27f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Validation Loss: 0.663\n",
            "Final Validation F1-score: 0.77\n",
            "Final Validation Precision: 0.79\n",
            "Final Validation Recall: 0.75\n",
            "Test Loss: 0.692\n",
            "Test F1-score: 0.76\n",
            "Test Precision: 0.79\n",
            "Test Recall: 0.74\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el mejor modelo entrenado durante la b√∫squeda en cuadr√≠cula\n",
        "best_model.load_state_dict(torch.load('best_model_exp2.pt'))\n",
        "\n",
        "# Enviamos el modelo a CUDA si est√° disponible\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de prueba\n",
        "test_loss, test_precision, test_recall, test_f1 = evaluate(\n",
        "    best_model, dataloader_test, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "print(f'Test F1-score: {test_f1:.2f}')\n",
        "print(f'Test Precision: {test_precision:.2f}')\n",
        "print(f'Test Recall: {test_recall:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqymD_Nuej41"
      },
      "source": [
        "#### An√°lisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88tsCz4eej41"
      },
      "source": [
        "Los resultados del experimento con la b√∫squeda en grilla de hiperpar√°metros demuestran una mejora significativa en el rendimiento general del modelo de Reconocimiento de Entidades Nombradas (NER) comparado con el modelo baseline. La configuraci√≥n √≥ptima identificada a trav√©s del experimento incluy√≥ una arquitectura bidireccional con una dimensi√≥n de embedding reducida a 200, la misma dimensi√≥n de las capas LSTM de 256, y un menor n√∫mero de capas LSTM, reduciendo de tres a dos.\n",
        "\n",
        "Comparaci√≥n de Rendimiento:\n",
        "\n",
        "El modelo baseline logr√≥ un F1-score de 0.71, precisi√≥n de 0.73 y recall de 0.70. Esto se obtuvo con una configuraci√≥n no bidireccional, una dimensi√≥n de embedding de 300, y tres capas LSTM.\n",
        "El modelo optimizado, en cambio, alcanz√≥ un F1-score de 0.76, con una precisi√≥n de 0.79 y un recall de 0.74. Esto representa un aumento de 5 puntos porcentuales en F1-score, 6 puntos en precisi√≥n y 4 puntos en recall.\n",
        "\n",
        "En cuanto al impacto de los Hiperpar√°metros:\n",
        "\n",
        "- Bidireccionalidad: La inclusi√≥n de la bidireccionalidad en el modelo optimizado fue decisiva para mejorar su capacidad de contextualizaci√≥n de las entidades en la secuencia de texto. Al procesar la informaci√≥n en ambas direcciones, el modelo pudo captar mejor las dependencias a largo plazo, lo que es especialmente √∫til en la tarea de NER donde el contexto puede influir significativamente en la clasificaci√≥n de las entidades.\n",
        "\n",
        "- Dimensi√≥n de Embedding: Reducir la dimensi√≥n de los embeddings de 300 a 200, contrario a la intuici√≥n inicial, no deterior√≥ el rendimiento. Esta reducci√≥n puede haber ayudado a evitar el overfitting, permitiendo que el modelo generalizara mejor con menos par√°metros a ajustar. Esto indica que una representaci√≥n m√°s compacta fue suficiente para captar las caracter√≠sticas relevantes del texto.\n",
        "\n",
        "-N√∫mero de Capas LSTM: Reducir el n√∫mero de capas de tres a dos tambi√©n contribuy√≥ a la mejora del rendimiento. En algunos casos, modelos m√°s simples con menos capas pueden ser m√°s efectivos, especialmente cuando el tama√±o del dataset, como en este caso, no justifica una mayor complejidad. Esto puede ayudar a reducir el riesgo de overfitting y mejorar la velocidad de entrenamiento.\n",
        "\n",
        "El experimento subraya la importancia de la configuraci√≥n adecuada de los hiperpar√°metros y c√≥mo ajustes aparentemente contra intuitivos pueden resultar en mejoras sustanciales. A pesar de reducir la dimensionalidad de los embeddings y el n√∫mero de capas LSTM, la adici√≥n de la bidireccionalidad permiti√≥ una mejora notable en todas las m√©tricas de evaluaci√≥n. Este an√°lisis sugiere que para futuros trabajos, explorar configuraciones que equilibren bien la complejidad y la capacidad de generalizaci√≥n del modelo puede ofrecer resultados a√∫n mejores. Adem√°s, experimentar con diferentes tasas de dropout y otras t√©cnicas de regularizaci√≥n podr√≠a proporcionar m√°s perspectivas sobre c√≥mo optimizar el rendimiento del modelo de NER en contextos cl√≠nicos y m√°s all√°."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcPtyefAenbW"
      },
      "source": [
        "### **Experimento 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYUFyLMJenbX"
      },
      "source": [
        "#### Explicaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuI3TaAhenbY"
      },
      "source": [
        "Este experimento propone reemplazar la arquitectura LSTM utilizada en el modelo baseline por una arquitectura GRU (Gated Recurrent Unit). Las GRU, siendo m√°s simples estructuralmente que las LSTM, pueden ofrecer un rendimiento similar o incluso superior en ciertas tareas de procesamiento de lenguaje natural debido a su eficiencia en el manejo de secuencias de datos y menor complejidad de par√°metros. Adem√°s de cambiar la arquitectura de la red, implementaremos t√©cnicas de optimizaci√≥n de hiperpar√°metros mediante una grilla de b√∫squeda para determinar la mejor configuraci√≥n posible en t√©rminos de dimensiones ocultas, n√∫mero de capas, bidireccionalidad y dropout. Tambi√©n aumentaremos el n√∫mero de √©pocas para el entrenamiento pero integraremos la t√©cnica de early stopping para evitar el sobreajuste, asegurando as√≠ que el modelo generalice mejor en datos no vistos. Este enfoque se espera que supere al modelo baseline en t√©rminos de precisi√≥n, recall y puntuaci√≥n F1. Este experimento es relevante ya que busca explorar c√≥mo las diferencias estructurales entre LSTM y GRU afectan el rendimiento en una tarea compleja de NER, proporcionando insights sobre la configuraci√≥n √≥ptima de hiperpar√°metros en modelos de redes neuronales recurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "800wWbuGenbY"
      },
      "source": [
        "#### Implementaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zON_wa1penbY",
        "outputId": "524fc0d9-1f9e-4654-a962-c53d8242a38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.5, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.01, 'n_layers': 2, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 1, 'num_epochs': 10}\n",
            "{'batch_size': 22, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'learning_rate': 0.001, 'n_layers': 2, 'num_epochs': 10}\n",
            "Mejor modelo guardado con F1: 0.7720945578221856\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import copy\n",
        "\n",
        "# Definir la red usando GRU\n",
        "class NER_GRU(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [sent len, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [sent len, batch size, emb dim]\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        # outputs = [sent len, batch size, hid dim * num directions]\n",
        "        predictions = self.fc(outputs)\n",
        "        # predictions = [sent len, batch size, output dim]\n",
        "        return predictions\n",
        "\n",
        "# Par√°metros para la b√∫squeda en grilla\n",
        "param_grid = {\n",
        "    'hidden_dim': [128, 256],\n",
        "    'n_layers': [1, 2],\n",
        "    'bidirectional': [True, False],\n",
        "    'dropout': [0.5, 0.2],\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'num_epochs': [10],\n",
        "    'batch_size': [22]\n",
        "}\n",
        "\n",
        "# Funci√≥n de early stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "results = []\n",
        "\n",
        "# Bucle para la b√∫squeda en grilla\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(params)\n",
        "    model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, params['hidden_dim'], OUTPUT_DIM, params['n_layers'], params['bidirectional'], params['dropout'], PAD_IDX)\n",
        "    model.apply(init_weights)\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
        "    criterion = criterion.to(device)\n",
        "    early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "    for epoch in range(params['num_epochs']):\n",
        "        train_loss, train_precision, train_recall, train_f1 = train(model, dataloader_train, optimizer, criterion)\n",
        "        valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, dataloader_dev, criterion)\n",
        "        if early_stopping(valid_loss):\n",
        "            break\n",
        "\n",
        "    if valid_f1 > best_f1:\n",
        "        best_f1 = valid_f1\n",
        "        best_model = copy.deepcopy(model)\n",
        "        best_params = params\n",
        "\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'f1': valid_f1,\n",
        "        'precision': valid_precision,\n",
        "        'recall': valid_recall\n",
        "    })\n",
        "\n",
        "# Seleccionamos el mejor modelo\n",
        "model = best_model\n",
        "\n",
        "# Guardar el modelo\n",
        "torch.save(model.state_dict(), 'best_gru_model.pt')\n",
        "print(\"Mejor modelo guardado con F1:\", best_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlY3Jqu-aKT1",
        "outputId": "fdaf68b8-844d-4254-bf92-70fc99e335c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hiperpar√°metros guardados en best_gru_model_hyperparameters.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# Nombre del archivo donde se guardar√°n los hiperpar√°metros\n",
        "filename = 'best_gru_model_hyperparameters.json'\n",
        "\n",
        "# Guardar los hiperpar√°metros en un archivo JSON\n",
        "with open(filename, 'w') as file:\n",
        "    json.dump(best_params, file, indent=4)\n",
        "\n",
        "print(f'Hiperpar√°metros guardados en {filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBzB3SlenbZ"
      },
      "source": [
        "#### Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w69TMqoenba",
        "outputId": "051f0597-70e4-4ad9-eb46-6ad6854e8fe5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ignacio/miniconda3/envs/nlp_t3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
            "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.699 | Test F1: 0.76 | Test Precision: 0.79 | Test Recall: 0.73\n"
          ]
        }
      ],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('best_gru_model.pt'))\n",
        "\n",
        "# Evaluamos el set de prueba con el modelo final\n",
        "test_loss, test_precision, test_recall, test_f1 = evaluate(model, dataloader_test, criterion)\n",
        "\n",
        "print(\n",
        "    f'Test Loss: {test_loss:.3f} | Test F1: {test_f1:.2f} | Test Precision: {test_precision:.2f} | Test Recall: {test_recall:.2f}'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxQkQt1Benba"
      },
      "source": [
        "#### An√°lisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT-giD-Tenbb"
      },
      "source": [
        "El experimento con el modelo GRU ha demostrado ser bastante prometedor, superando al baseline con LSTM en todas las m√©tricas de evaluaci√≥n. La mejora en el rendimiento puede atribuirse a varios factores clave, entre los cuales la configuraci√≥n de hiperpar√°metros y el cambio de arquitectura son los m√°s significativos.\n",
        "\n",
        "- Comparaci√≥n de Arquitecturas y Configuraciones\n",
        "\n",
        "    El baseline utilizando LSTM ten√≠a configuraciones de hidden_dim=256, n_layers=3, y dropout=0, con un enfoque unidireccional. Por otro lado, el modelo GRU que mostr√≥ los mejores resultados utiliz√≥ una configuraci√≥n de hidden_dim=128, n_layers=2, y dropout=0.2, y crucialmente, fue bidireccional.\n",
        "\n",
        "    - Dimensiones y Capas Ocultas: Reducir la dimensi√≥n oculta y el n√∫mero de capas en el modelo GRU, en comparaci√≥n con el LSTM, podr√≠a haber contribuido a evitar el sobreajuste y mejorar la generalizaci√≥n, especialmente dado el mayor n√∫mero de par√°metros que requiere una configuraci√≥n bidireccional.\n",
        "\n",
        "    - Bidireccionalidad: La inclusi√≥n de la bidireccionalidad en el GRU permiti√≥ que el modelo capturara contextos tanto pasados como futuros de la secuencia, lo cual es una ventaja significativa para tareas de etiquetado secuencial como NER, donde la informaci√≥n contextual es crucial.\n",
        "\n",
        "    - Dropout: La adici√≥n de un dropout de 0.2 ayud√≥ a mitigar el sobreajuste durante el entrenamiento, permitiendo que el modelo generalizara mejor a nuevos datos. Esto contrasta con el baseline que no utilizaba dropout, lo que potencialmente podr√≠a llevar a un modelo que memoriza el conjunto de entrenamiento pero no generaliza bien.\n",
        "\n",
        "- Resultados de Evaluaci√≥n\n",
        "\n",
        "    - LSTM (Baseline): Logr√≥ una precisi√≥n de 0.73, un recall de 0.70, y un F1-score de 0.71 con una p√©rdida de 0.421.\n",
        "    - GRU (Optimizado): Mejor√≥ significativamente, alcanzando una precisi√≥n de 0.79, un recall de 0.73, y un F1-score de 0.76, aunque con una p√©rdida m√°s alta de 0.699.\n",
        "\n",
        "\n",
        "La elecci√≥n de GRU en este contexto particular ha demostrado ser bastante bueno, posiblemente debido a la simplicidad inherente de GRU, que tiene menos par√°metros que administrar y es m√°s f√°cil de optimizar con los recursos y el tama√±o de datos disponibles.\n",
        "\n",
        "\n",
        "- LSTM vs GRU\n",
        "\n",
        "En comparaci√≥n con el Experimento 2 donde se optimiz√≥ una LSTM, GRU tuvo los mismos resultados en las m√©tricas objetivo a excepci√≥n del recall donde LSTM tuvo uno mas de recall con 0.74 vs 0.73, sin embargo, estos experimentos son insuficientes para decantar por uno o por otro, debido a que se entren√≥ por una cantidad muy limitada  de √©pocas y una cantidad limitada de opciones de valores para los hiperpar√°metros, sobre todo para la LSTM donde no se prob√≥ el dropout, sin embargo, al tener resultados tan similares, se puede declarar como ganador provisorio al GRU al ser un modelo mucho m√°s simple y r√°pido de entrenar."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}